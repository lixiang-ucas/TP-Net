{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#multi-scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pylab as plt\n",
    "import glob\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "# import scipy\n",
    "import sugartensor as tf\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import UtilsLJ\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "sys.path.append('./src/')\n",
    "from model_multi import *\n",
    "from utils import * \n",
    "\n",
    "#sys.path.append('./PointSetGeneration/depthestimate')\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
    "\n",
    "#About Data\n",
    "\n",
    "batSize=2\n",
    "maxStep=100000 # fixed with learningRate and learningRateDecay\n",
    "learningRate=1e-2\n",
    "decayStep = 30000\n",
    "learningRateDecay=0.1\n",
    "lrType=1 #learning rate type\n",
    "lr2Period=5\n",
    "adam_beta1=0.9 # check adam optimization\n",
    "adam_beta2=0.99\n",
    "\n",
    "maxGradient = 1\n",
    "\n",
    "conWeightVar=['train/FeastNet','global_step']  #'FeastNet','global_step'# variables to be loaded\n",
    "saveStep=80/batSize*10 # frequency to save weight\n",
    "maxKeepWeights=2000 # how many records to save (for disk)\n",
    "stepsContinue=-1  # from which steps continu.\n",
    "#For Debug and results printing\n",
    "keepProb=0.99999\n",
    "# Totally dosen't work if put dropout after max-pool layer. \n",
    "printStep=80/batSize\n",
    "ss=6890\n",
    "dat=\"FEAST_FAUST-wen-LLE-Single-V6-18-Rerun\"\n",
    "Mconv = 32\n",
    "\n",
    "lle_coef = 0.001\n",
    "reg_coef = 1e-5\n",
    "dirSave=\"./TFweights/Simple{}_Mconv_{}_Ss_{}_Bs_{}_lrT_{}_LR_{}_LRdecay_{}_Dkp_{}_Batchsize_{}_MaxGradient_{}_LLEcoef_{}_Regcoef{}\".format(\n",
    "    dat, Mconv, ss, batSize, lrType, learningRate, learningRateDecay,keepProb,batSize,maxGradient,lle_coef,reg_coef)\n",
    "\n",
    "#conWeightPath=\"./TFweights/Simple2Dto3DMGPU_NR_1_FBN_0_Ss_1024_Bs_32_lrT_1_LR_0.001_LRdecay_0.999_Dkp_0.99999\"\n",
    "conWeightPath=\"\"\n",
    "\n",
    "\n",
    "RSW=8\n",
    "RSD=8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6890\n",
    "ARCHITECTURE = 0\n",
    "# DATASET_PATH = FLAGS.dataset_path\n",
    "# RESULTS_PATH = FLAGS.results_path\n",
    "# NUM_ITERATIONS = FLAGS.num_iterations\n",
    "# NUM_INPUT_CHANNELS = FLAGS.num_input_channels\n",
    "# LEARNING_RATE = FLAGS.learning_rate\n",
    "# NUM_POINTS = FLAGS.num_points\n",
    "# NUM_CLASSES = FLAGS.num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drawPlot(x,y,name):\n",
    "    plt.rcParams['savefig.dpi'] = 300 \n",
    "#    xmaxorLocator = MultipleLocator(1) \n",
    "#    plt.gca().xaxis.set_major_locator(xmaxorLocator)\n",
    "#    plt.ylim(0,50)\n",
    "    plt.plot(np.arange(0,len(x))*4,x,'k-',alpha=1,label='Train max: '+str(round(max(x),3))+', min: '+str(round(min(x),3)))\n",
    "    plt.plot(np.arange(0,len(y)),y,'r-',alpha=1,label='Test max: '+str(round(max(y),3))+', min: '+str(round(min(y),3)))\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch',fontsize=9)\n",
    "    plt.ylabel(name+' value',fontsize=9)\n",
    "    plt.savefig(dirSave+\"/\"+name+\".jpg\",bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y_,adj=UtilsLJ.read_from_tfrecords(\n",
    "['./data/FAUST_FEAST_TRAIN_normilize.tfrecords'],['xyz','label','adj'],batSize,\n",
    "                              [[6890,3],[6890],[6890,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_val = tf.sg_print(x)\n",
    "adj_val = tf.sg_print(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adj_val.min(),adj_val.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(888)\n",
    "    print(\"*****************************************\")\n",
    "    print(\"Training started with random seed: {}\".format(RSW))\n",
    "    print(\"Batch started with random seed: {}\".format(RSD))\n",
    "    \n",
    "\n",
    "    x,y_,adj=UtilsLJ.read_from_tfrecords(\n",
    "    ['./data/FAUST_FEAST_TRAIN_normilize.tfrecords'],['xyz','label','adj'],batSize,\n",
    "                                  [[6890,3],[6890],[6890,6]])\n",
    "    \n",
    "\n",
    "    x_t,y_t,adj_t=UtilsLJ.read_from_tfrecords(\n",
    "    ['./data/FAUST_FEAST_TEST_normilize.tfrecords'],['xyz','label','adj'],batSize,\n",
    "                                  [[6890,3],[6890],[6890,6]],shuffle=False)\n",
    "    \n",
    "    \n",
    "    y_ = tf.cast(y_, dtype=tf.int64)\n",
    "    y_t = tf.cast(y_t, dtype=tf.int64)\n",
    "    adj = tf.cast(adj, dtype=tf.int32)\n",
    "    adj_t = tf.cast(adj_t, dtype=tf.int32)\n",
    "    \n",
    "    global_step = tf.Variable(1, trainable=False,name='global_step')\n",
    "\n",
    "    # get model and loss\n",
    "    with tf.variable_scope(\"train\") as scope1:\n",
    "        y_conv,lle_loss = get_model_multi_lle(x, adj, NUM_CLASSES, ARCHITECTURE, Mconv)\n",
    "        scope1.reuse_variables()\n",
    "        y_conv_t,lle_loss_t = get_model_multi_lle(x_t, adj_t, NUM_CLASSES, ARCHITECTURE, Mconv)\n",
    "    \n",
    "    batch = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Standard classification loss\n",
    "    Loss = tf.reduce_mean(tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_, logits=y_conv), axis=1))\n",
    "    Loss = Loss+ lle_loss * lle_coef\n",
    "    reg_losses = sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "    Loss = Loss+ reg_losses*reg_coef\n",
    "    predictions = tf.argmax(y_conv, 2)\n",
    "    correct_prediction = tf.equal(predictions, y_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    Loss_t = tf.reduce_mean(tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_t, logits=y_conv_t), axis=1))\n",
    "    predictions_t = tf.argmax(y_conv_t, 2)\n",
    "    correct_prediction_t = tf.equal(tf.argmax(y_conv_t,2), y_t)\n",
    "    accuracy_t = tf.reduce_mean(tf.cast(correct_prediction_t, tf.float32))\n",
    "    \n",
    "    print(Loss.shape, accuracy.shape, accuracy_t.shape)\n",
    "    \n",
    "    # Add summary writers\n",
    "    #merged = tf.merge_all_summaries()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Learning Rate****************************************************************************\n",
    "    \n",
    "    lr = tf.train.exponential_decay(learningRate, global_step,\n",
    "                                                  decayStep, learningRateDecay, staircase=True) \n",
    "    lr=tf.clip_by_value(lr, 1e-6,1)\n",
    "    \n",
    "    # Optimization Algo************************************************************************\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr,\n",
    "                                                    beta1=adam_beta1,\n",
    "                                                    beta2=adam_beta2\n",
    "                                                   )\n",
    "    gradients, variables = zip(*optimizer.compute_gradients(Loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "    train_step = optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=int(maxKeepWeights))\n",
    "   \n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                               tf.local_variables_initializer())\n",
    "    \n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "    tf.summary.scalar('loss_train', Loss)\n",
    "    tf.summary.scalar('loss_train_lle', lle_loss)\n",
    "    tf.summary.scalar('loss_train_reg', reg_losses)\n",
    "#     tf.summary.scalar('loss_test', Loss_t)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "#     tf.summary.scalar('accuracy_test', accuracy_t)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Continue Training************************************************************************\n",
    "    if len(conWeightPath)>0:\n",
    "        print(\"Continue Training...\")\n",
    "        tmp_var_list={}\n",
    "        if len(conWeightVar)==0:\n",
    "            print(\"For all variables\")\n",
    "            globals()['conWeightVar']={''}\n",
    "        else:\n",
    "            print(\"Training variables: {}\".format(conWeightVar))\n",
    "            \n",
    "        for j in conWeightVar: \n",
    "            for i in tf.global_variables():\n",
    "                if i.name.startswith(j):\n",
    "                    tmp_var_list[i.name[:-2]] = i\n",
    "                    \n",
    "        saver1=tf.train.Saver(tmp_var_list) \n",
    "    # For Testing Dosent matther****************************************************************   \n",
    "    \n",
    "    \n",
    "    # Training**********************************************************************************    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(os.path.join(dirSave, 'train'),\n",
    "                                  sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(os.path.join(dirSave, 'test'))\n",
    "\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        # Read Weight******************************\n",
    "        if len(conWeightPath)>0:\n",
    "            print(conWeightPath)\n",
    "            if stepsContinue==-1:            \n",
    "                STEPS=sorted([int(i.split(\"/\")[-1].split(\".\")[1].split(\"-\")[-1]) for i in glob.glob(conWeightPath+\"/*meta\")])\n",
    "                globals()['stepsContinue']=STEPS[-1]\n",
    "                \n",
    "            wtt=glob.glob(conWeightPath+\"/*{}*meta\".format(stepsContinue))[0][:-5]\n",
    "            print(\"Reading Weight:{}\".format(wtt))\n",
    "            saver1.restore(sess,wtt)\n",
    "            print('Weight is successfully updated from: {}'.format(wtt))  \n",
    "        #*******************************************    \n",
    "        \n",
    "        stepst = sess.run(global_step)\n",
    "        \n",
    "        train_acc=[]\n",
    "        test_acc=[]\n",
    "        train_loss_list=[]\n",
    "        test_loss_list=[]\n",
    "\n",
    "        acc_epoch=[]\n",
    "        acc_epoch_t=[]\n",
    "        \n",
    "        train_loss_epoch=0\n",
    "        test_loss_epoch=0\n",
    "        \n",
    "        best_val_acc = 0\n",
    "        bast_val_step = 0\n",
    "        \n",
    "        for t in tqdm(range(stepst,int(maxStep)+1)):\n",
    "            \n",
    "            step, summary, _, llr, Ltr, Ltr_lle, Ltr_reg = sess.run([global_step, merged, train_step, lr, Loss, lle_loss, reg_losses]) \n",
    "\n",
    "            \n",
    "            if t==stepst:\n",
    "                Ls=1\n",
    "            else:\n",
    "                Ls=0.99*Ls+0.01*Ltr\n",
    "                \n",
    "            #RL=0.999*RL+0.001*regL\n",
    "            \n",
    "            # Calculate Accuracy******************************\n",
    "            train_loss, pred, train_y= sess.run([ Loss, y_conv, y_]) \n",
    "            train_correct_prediction = np.equal(np.argmax(pred,2), train_y)\n",
    "            acc_epoch.append(train_correct_prediction)\n",
    "            train_loss_epoch=train_loss_epoch + train_loss/(80/batSize)\n",
    "            \n",
    "            \n",
    "            test_loss, pred_t, test_y= sess.run([ Loss_t,y_conv_t, y_t]) \n",
    "            test_correct_prediction = np.equal(np.argmax(pred_t,2), test_y)\n",
    "            acc_epoch_t.append(test_correct_prediction)\n",
    "            test_loss_epoch=test_loss_epoch + train_loss/(20/batSize)\n",
    "            #*******************************************   \n",
    "            \n",
    "            # train epoch ******************************\n",
    "            if t%(80/batSize)==0:            \n",
    "                train_acc_epoch = np.mean(acc_epoch)\n",
    "                train_acc.append(train_acc_epoch)\n",
    "                acc_epoch=[]\n",
    "                print('learning rate: %f, train loss: %f, train acc: %f, lle_loss: %f, reg_loss: %f' % (llr, train_loss_epoch, train_acc_epoch, Ltr_lle, Ltr_reg))\n",
    "#                 print('gt_t: ', train_y, 'pred: ', np.argmax(pred,2))\n",
    "                train_loss_list.append(train_loss_epoch)\n",
    "                train_loss_epoch=0\n",
    "                train_writer.add_summary(summary, step)\n",
    "            #*******************************************   \n",
    "            \n",
    "            # test epoch ******************************\n",
    "            if t%(20/batSize)==0:            \n",
    "                test_acc_epoch = np.mean(acc_epoch_t)\n",
    "                if test_acc_epoch>best_val_acc:\n",
    "                    best_val_acc = test_acc_epoch\n",
    "                    bast_val_step = t\n",
    "                test_acc.append(test_acc_epoch)\n",
    "                acc_epoch_t=[]\n",
    "                print('learning rate: %f, test loss: %f, test acc: %f' % (llr, test_loss_epoch, test_acc_epoch))\n",
    "#                 print('gt_t: ', test_y, 'pred_t: ', np.argmax(pred_t,2))\n",
    "                test_loss_list.append(test_loss_epoch)\n",
    "                test_loss_epoch=0\n",
    "            #*******************************************  \n",
    "                \n",
    "#                 print('y_conv_val:', y_conv_val[0,:5,:])\n",
    "            if t% 1000==0:\n",
    "                if not os.path.exists(dirSave):\n",
    "                    os.makedirs(dirSave)\n",
    "                    \n",
    "                with open(dirSave + '/result.txt', 'a') as f:\n",
    "                    f.write(str(llr)+\"*\"+str(Ls)+\"\\n\")\n",
    "                    f.close()\n",
    "                \n",
    "                with open(dirSave + '/acc.txt', 'w') as f2:\n",
    "                    f2.write(str(train_acc)+\"\\n\"+str(test_acc)+\"\\n\")\n",
    "                    f2.close()\n",
    "\n",
    "            if (t % saveStep==0):\n",
    "                if not os.path.exists(dirSave):\n",
    "                    os.makedirs(dirSave)\n",
    "                saver.save(sess, dirSave + '/model.ckpt', global_step=t)\n",
    "                \n",
    "                \n",
    "                if (len(train_loss_list)!=0) and (len(test_loss_list)!=0) and (len(train_acc)!=0) and (len(test_acc)!=0):\n",
    "                    \n",
    "                    drawPlot(train_loss_list,test_loss_list,\"Loss\")\n",
    "                    drawPlot(train_acc,test_acc,\"Accuracy\")\n",
    "        \n",
    "        \n",
    "        coord.request_stop()\n",
    "        coord.join(threads)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# conWeightPath = './TFweights/SimpleFAUST-V2_NR_1_FBN_0_Ss_2048_Bs_8_lrT_1_LR_0.001_LRdecay_0.995_Dkp_0.99999/'\n",
    "\n",
    "# stepsContinue = 240000\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(bast_val_step, best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(888)\n",
    "    print(\"*****************************************\")\n",
    "    print(\"Training started with random seed: {}\".format(RSW))\n",
    "    print(\"Batch started with random seed: {}\".format(RSD))\n",
    "    \n",
    "\n",
    "#     x,y_,adj=UtilsLJ.read_from_tfrecords(\n",
    "#     ['./data/FAUST_FEAST_TRAIN_normilize.tfrecords'],['xyz','label','adj'],batSize,\n",
    "#                                   [[6890,3],[6890],[6890,6]])\n",
    "    \n",
    "\n",
    "    x_t,y_t,adj_t=UtilsLJ.read_from_tfrecords(\n",
    "    ['./data/FAUST_FEAST_TEST_normilize.tfrecords'],['xyz','label','adj'],batSize,\n",
    "                                  [[6890,3],[6890],[6890,6]],shuffle=False)\n",
    "    \n",
    "    \n",
    "#     y_ = tf.cast(y_, dtype=tf.int64)\n",
    "    y_t = tf.cast(y_t, dtype=tf.int64)\n",
    "#     adj = tf.cast(adj, dtype=tf.int32)\n",
    "    adj_t = tf.cast(adj_t, dtype=tf.int32)\n",
    "    \n",
    "    global_step = tf.Variable(1, trainable=False,name='global_step')\n",
    "\n",
    "    # get model and loss\n",
    "    with tf.variable_scope(\"train\") as scope1:\n",
    "#         y_conv = get_model(x, adj, NUM_CLASSES, ARCHITECTURE, Mconv)\n",
    "#         scope1.reuse_variables()\n",
    "        y_conv_t,lle_loss_t = get_model_multi_lle(x_t, adj_t, NUM_CLASSES, ARCHITECTURE, Mconv)\n",
    "    \n",
    "    batch = tf.Variable(0, trainable=False)\n",
    "\n",
    "#     Standard classification loss\n",
    "#     Loss = tf.reduce_mean(tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_, logits=y_conv), axis=1))\n",
    "#     predictions = tf.argmax(y_conv, 2)\n",
    "#     correct_prediction = tf.equal(predictions, y_)\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    Loss_t = tf.reduce_mean(tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_t, logits=y_conv_t), axis=1))\n",
    "    predictions_t = tf.argmax(y_conv_t, 2)\n",
    "    correct_prediction_t = tf.equal(tf.argmax(y_conv_t,2), y_t)\n",
    "    accuracy_t = tf.reduce_mean(tf.cast(correct_prediction_t, tf.float32))\n",
    "    \n",
    "#     print(accuracy.shape, accuracy_t.shape)\n",
    "\n",
    "    #Learning Rate****************************************************************************\n",
    "    \n",
    "#     lr = tf.train.exponential_decay(learningRate, global_step,\n",
    "#                                                   decayStep, learningRateDecay, staircase=True) \n",
    "#     lr=tf.clip_by_value(lr, 1e-6,1)\n",
    "    \n",
    "#     # Optimization Algo************************************************************************\n",
    "#     train_step = tf.train.AdamOptimizer(learning_rate=lr,\n",
    "#                                                     beta1=adam_beta1,\n",
    "#                                                     beta2=adam_beta2\n",
    "#                                                    ).minimize(Loss,global_step=global_step)\n",
    "    \n",
    "#     saver = tf.train.Saver(max_to_keep=int(maxKeepWeights))\n",
    "   \n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                               tf.local_variables_initializer())\n",
    "    \n",
    "    # Continue Training************************************************************************\n",
    "    if len(conWeightPath)>0:\n",
    "        print(\"Continue Training...\")\n",
    "        tmp_var_list={}\n",
    "        if len(conWeightVar)==0:\n",
    "            print(\"For all variables\")\n",
    "            globals()['conWeightVar']={''}\n",
    "        else:\n",
    "            print(\"Training variables: {}\".format(conWeightVar))\n",
    "            \n",
    "        for j in conWeightVar: \n",
    "            for i in tf.global_variables():\n",
    "                if i.name.startswith(j):\n",
    "                    tmp_var_list[i.name[:-2]] = i\n",
    "                    \n",
    "        saver1=tf.train.Saver(tmp_var_list) \n",
    "    # For Testing Dosent matther****************************************************************   \n",
    "    \n",
    "    \n",
    "    # Training**********************************************************************************    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        # Read Weight******************************\n",
    "        if len(conWeightPath)>0:\n",
    "            print(conWeightPath)\n",
    "            if stepsContinue==-1:            \n",
    "                STEPS=sorted([int(i.split(\"/\")[-1].split(\".\")[1].split(\"-\")[-1]) for i in glob.glob(conWeightPath+\"/*meta\")])\n",
    "                globals()['stepsContinue']=STEPS[-1]\n",
    "                \n",
    "            wtt=glob.glob(conWeightPath+\"/*{}*meta\".format(stepsContinue))[0][:-5]\n",
    "            print(\"Reading Weight:{}\".format(wtt))\n",
    "            saver1.restore(sess,wtt)\n",
    "            print('Weight is successfully updated from: {}'.format(wtt))  \n",
    "        #*******************************************    \n",
    "        \n",
    "        stepst = sess.run(global_step)\n",
    "        \n",
    "        cord_list = []\n",
    "        pred_list = []\n",
    "        gt_list = []\n",
    "        acc_list = []\n",
    "        prob_list = []\n",
    "        for t in tqdm(range(20//batSize)):\n",
    "            cord, pred_cur, gt_cur, acc_cur, y_conv_t_cur = sess.run([x_t, predictions_t,y_t,accuracy_t, y_conv_t])\n",
    "            cord_list.append(cord)\n",
    "            pred_list.append(pred_cur)\n",
    "            gt_list.append(gt_cur)\n",
    "            acc_list.append(acc_cur)\n",
    "            prob_list.append(y_conv_t_cur)\n",
    "        cord_list = np.concatenate(cord_list,axis=0)\n",
    "        pred_list = np.concatenate(pred_list,axis=0)\n",
    "        gt_list = np.concatenate(gt_list,axis=0)\n",
    "        prob_list = np.concatenate(prob_list, axis=0)\n",
    "        print('acc: %f' % (np.mean(acc_list)))\n",
    "        \n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        \n",
    "        return cord_list, pred_list, gt_list, acc_list, prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(os.listdir(dirSave))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stepsContinue=25200\n",
    "conWeightPath=dirSave\n",
    "cord_list, pred_list, gt_list, acc_list, prob_list = test()\n",
    "print('acc: %f' % (np.mean(acc_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "10000: 0.982, 16000:0.995, 18000:0.989, 25200: 0.97\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls -l ./TFweights/SimpleFEAST_FAUST-wen-LLE-Single-V6-18_Mconv_32_Ss_6890_Bs_2_lrT_1_LR_0.01_LRdecay_0.1_Dkp_0.99999_Batchsize_2_MaxGradient_1_LLEcoef_0.001_Regcoef1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_list[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.mkdir('Result/')\n",
    "np.save('Result/pred_V6-18-prob.npy',(pred_list,gt_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calculate mean distance error\n",
    "dist_list = []\n",
    "for s in range(cord_list.shape[0]):\n",
    "    gt_xyz = cord_list[s][gt_list[s]]\n",
    "    pred_xyz = cord_list[s][pred_list[s]]\n",
    "    dist = np.sqrt(np.sum((gt_xyz-pred_xyz)**2, axis=1))\n",
    "    dist = np.mean(dist)\n",
    "    dist_list.append(dist)\n",
    "dist_mean = np.mean(dist_list)\n",
    "print('mean distance error: ', dist_mean)\n",
    "np.save(os.path.join(dirSave,'dist_mean.npy'),dist_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calculate accuracy under different distance threshod\n",
    "#calculate mean distance error\n",
    "acc_list = []\n",
    "thr_list = np.arange(0,0.1,0.005)\n",
    "for s in range(cord_list.shape[0]):\n",
    "    for thr in thr_list:\n",
    "        gt_xyz = cord_list[s][gt_list[s]]\n",
    "        pred_xyz = cord_list[s][pred_list[s]]\n",
    "        dist = np.sqrt(np.sum((gt_xyz-pred_xyz)**2, axis=1))\n",
    "        acc = np.count_nonzero(dist<=thr)/float(ss)\n",
    "        acc_list.append(acc)\n",
    "acc_list = np.array(acc_list)\n",
    "acc_list = acc_list.reshape((cord_list.shape[0], len(thr_list)))\n",
    "acc_thr = np.mean(acc_list,axis=0)\n",
    "print('mean accuracy under different distance threshod: ', acc_thr)\n",
    "fig = plt.figure()\n",
    "plt.plot(thr_list, acc_thr)\n",
    "plt.xlim([0,0.101])\n",
    "plt.ylim([0.9,1.01])\n",
    "plt.xlabel('Geodesic error (% diameter)')\n",
    "plt.ylabel('% correspondences')\n",
    "plt.savefig(os.path.join(dirSave, 'acc.jpg'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pred_list,gt_list = np.load('Result/pred_V6-18.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_list.shape,gt_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Xiang_Li]",
   "language": "python",
   "name": "conda-env-Xiang_Li-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
